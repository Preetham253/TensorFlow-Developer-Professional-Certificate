{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOKoipS9ctLqfQsfzvxvWlO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Training with ImageDataGenerator"],"metadata":{"id":"E-t7gv-JJhkx"}},{"cell_type":"markdown","source":[" I will build a train a model on the Horses or Humans dataset. This contains over a thousand images of horses and humans with varying poses and filesizes."],"metadata":{"id":"J6_ripJ1JpYc"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"L-2CnxtrnWKj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717704291717,"user_tz":-330,"elapsed":910,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}},"outputId":"3af794df-b67a-4305-d49e-02dcdbd06f1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-06-06 20:04:49--  https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.2.207, 74.125.137.207, 2607:f8b0:4023:c0b::cf, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.2.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 149574867 (143M) [application/zip]\n","Saving to: ‘horse-or-human.zip’\n","\n","horse-or-human.zip  100%[===================>] 142.65M   155MB/s    in 0.9s    \n","\n","2024-06-06 20:04:50 (155 MB/s) - ‘horse-or-human.zip’ saved [149574867/149574867]\n","\n"]}],"source":["#download the compressed dataset\n","!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"]},{"cell_type":"code","source":["#Unzip the data\n","import zipfile\n","\n","#unzip the file\n","local_zip = './horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('./horse-or-human')\n","zip_ref.close()"],"metadata":{"id":"G-sRm2f_J2ug","executionInfo":{"status":"ok","timestamp":1717704484602,"user_tz":-330,"elapsed":711,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["The contents of the .zip are extracted to the base directory ./horse-or-human, which in turn each contain horses and humans subdirectories."],"metadata":{"id":"74L_DtvPKpO3"}},{"cell_type":"markdown","source":["We do not explicitly label the images as horses or humans. You will use the ImageDataGenerator API instead -- and this is coded to automatically label images according to the directory names and structure. So, for example, you will have a 'training' directory containing a 'horses' directory and a 'humans' one. ImageDataGenerator will label the images appropriately for you, reducing a coding step."],"metadata":{"id":"QJqKqsYQK8wj"}},{"cell_type":"code","source":["import os\n","#Directory with our training horse pictures\n","train_horse_dir = os.path.join('./horse-or-human/horses')\n","\n","#Directory with our training human pictures\n","train_human_dir = os.path.join('./horse-or-human/humans')"],"metadata":{"id":"TgwpWs-oKl5x","executionInfo":{"status":"ok","timestamp":1717704743198,"user_tz":-330,"elapsed":7,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["train_horse_names = os.listdir(train_horse_dir)\n","print(train_horse_names[:10])\n","\n","train_human_names = os.listdir(train_human_dir)\n","print(train_human_names[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vws4uJ2vLlJR","executionInfo":{"status":"ok","timestamp":1717704860817,"user_tz":-330,"elapsed":5,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}},"outputId":"769af72c-f4d6-4e9b-ae8d-4e0d8f56dc8f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['horse36-2.png', 'horse36-4.png', 'horse14-0.png', 'horse01-2.png', 'horse08-7.png', 'horse04-8.png', 'horse10-3.png', 'horse38-2.png', 'horse32-8.png', 'horse21-2.png']\n","['human15-02.png', 'human16-20.png', 'human07-17.png', 'human01-14.png', 'human02-11.png', 'human16-29.png', 'human07-21.png', 'human12-00.png', 'human09-27.png', 'human08-15.png']\n"]}]},{"cell_type":"code","source":["print('total training horse images:',len(os.listdir(train_horse_dir)))\n","print('total training human images:',len(os.listdir(train_human_dir)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_TK9-l3MB8o","executionInfo":{"status":"ok","timestamp":1717704950821,"user_tz":-330,"elapsed":7,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}},"outputId":"ee3d1ba4-33b6-42fd-b03b-63dd9993a844"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["total training horse images: 500\n","total training human images: 527\n"]}]},{"cell_type":"markdown","source":["Now take a look at a few pictures to get a better sense of what they look like.|"],"metadata":{"id":"T12WpEbQMcM7"}},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mp\n","#parameters for our graph; we will output images in a 4x4 configuration\n","nrows = 4\n","ncols = 4\n","\n","#index for iterating over images\n","pic_index = 0"],"metadata":{"id":"i94rLcR0MX7L","executionInfo":{"status":"ok","timestamp":1717705116741,"user_tz":-330,"elapsed":2,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Now, display a batch of 9 horse and 9 human pictures."],"metadata":{"id":"QzntcLtaNG7o"}},{"cell_type":"code","source":["#set up matplotlib fig, and sizeit to fit 4x4 pics\n","fig = plt.gcf()\n","fig.set_size_inches(ncols *4, nrows *4)\n","pic_index += 9\n","next_horse_pix = [os.path.join(train_horse_dir, fname)\n","                for fname in train_horse_names[pic_index-8:pic_index]]\n","next_human_pix = [os.path.join(train_human_dir, fname)\n","                  for fname in train_human_names[pic_index-8:pic_index]]\n","for i, img_path in enumerate(next_horse_pix + next_human_pix):\n","  #set up subplot; subplot indices start at 1\n","  sp =plt.subplot(nrows, ncols, i+1)\n","  sp.axis( 'off')#Donot show axis\n","\n","  img = mp.imread(img_path)\n","  plt.imshow(img)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1CY5VzKnhp7zPkBS1-yBWkaLTqMqTr3BB"},"id":"l0DcX2dCM8Pv","executionInfo":{"status":"ok","timestamp":1717705692912,"user_tz":-330,"elapsed":5016,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}},"outputId":"6474301b-d5ef-418e-c037-e481067d3a11"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Building a Small Model from Scratch"],"metadata":{"id":"_hywKrBwPes6"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras"],"metadata":{"id":"WQwTPDUiOlPL","executionInfo":{"status":"ok","timestamp":1717705799215,"user_tz":-330,"elapsed":6296,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["model = tf.keras.models.Sequential([\n","    #first convolution layer\n","    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300,300,3)),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    #Second convolution layer\n","    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    #Third convolutional layer\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    #Fourth convolutional layer\n","    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    #Fifth convolutional layer\n","    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    #Flatten the results to feed into a DNN\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    #only 1 output neuron, since it is a binary classification\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])"],"metadata":{"id":"tFXF0c4ePlgv","executionInfo":{"status":"ok","timestamp":1717706149238,"user_tz":-330,"elapsed":1091,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Arwz5J_tQ8T2","executionInfo":{"status":"ok","timestamp":1717706158178,"user_tz":-330,"elapsed":423,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}},"outputId":"025aef2e-c06b-44e9-d384-47d604e4e19f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 298, 298, 16)      448       \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 149, 149, 16)      0         \n"," D)                                                              \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 147, 147, 32)      4640      \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 73, 73, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 71, 71, 64)        18496     \n","                                                                 \n"," max_pooling2d_2 (MaxPoolin  (None, 35, 35, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 33, 33, 128)       73856     \n","                                                                 \n"," max_pooling2d_3 (MaxPoolin  (None, 16, 16, 128)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 14, 14, 128)       147584    \n","                                                                 \n"," max_pooling2d_4 (MaxPoolin  (None, 7, 7, 128)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten (Flatten)           (None, 6272)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               3211776   \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 513       \n","                                                                 \n","=================================================================\n","Total params: 3457313 (13.19 MB)\n","Trainable params: 3457313 (13.19 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.optimizers import RMSprop\n","model.compile(loss='binary_crossentropy',\n","              optimizer = RMSprop(learning_rate=0.001),\n","              metrics=['accuracy'])"],"metadata":{"id":"qik0m-qEQ-n5","executionInfo":{"status":"ok","timestamp":1717706441736,"user_tz":-330,"elapsed":425,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["#Data Preprocessing"],"metadata":{"id":"9yKf8GrgSFrT"}},{"cell_type":"markdown","source":["Next step is to set up the data generators that will read pictures in the source folders, convert them to float32 tensors, and feed them (with their labels) to the model. You'll have one generator for the training images and one for the validation images. These generators will yield batches of images of size 300x300 and their labels (binary)."],"metadata":{"id":"ZfUUPesYSTzn"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","#All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1/255)\n","\n","#Flow training images in batches of 128 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        './horse-or-human/', #This is the source directory for training images\n","        target_size=(300,300), #All images will be resized to 300x300\n","        batch_size=128,\n","        #Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLe_mS7uSD4H","executionInfo":{"status":"ok","timestamp":1717706854705,"user_tz":-330,"elapsed":402,"user":{"displayName":"Preetham K","userId":"01174741689971932660"}},"outputId":"2e4b2b4d-a920-43eb-bd4d-c5ee2a02a980"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1027 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["#Train the model\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=8,\n","    epochs=15,\n","    verbose=1\n",")"],"metadata":{"id":"8MLJMrLjTos9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model Prediction"],"metadata":{"id":"xewD9jZnUBjK"}},{"cell_type":"markdown","source":["Now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, upload them, and run them through the model, giving an indication of whether the object is a horse or a human."],"metadata":{"id":"9g2RrdB8UDy5"}},{"cell_type":"code","source":["import numpy as np\n","from google.colab import files\n","from tensorflow.keras.utils import load_img, img_to_array\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  #predicting images\n","  path = '/content/' + fn\n","  img = load_img(path, target_size=(300,300))\n","  x = img_to_array(img)\n","  x /= 255\n","  x = np.expand_dims(x, axis=0)\n","\n","  images = np.vstack([x])\n","  classes = model.predict(images, batch_size=10)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + 'is a human')\n","  else:\n","    print(fn + 'is a horse')"],"metadata":{"id":"Fq0z-aTbT83k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fNUDyzvRUgzZ"},"execution_count":null,"outputs":[]}]}